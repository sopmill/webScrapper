Sophia Miller   my_news_downloader.py

The purpose of this program is to scrap a website containing an article and output a new file full of just the article contents. No advertisements or other clutter is written into the new file, just the contents of the article. Using python and the python libarary, BeautifulSoup, I was able to accomplish this program. The program has a total of four functions as well as main. In main the first function, "process_urls_from_file" is called. This function is the main function that calls the other three functions in order to easily follow the code. The first function it calls is "download_content" which uses requests to download all the information from the given URL and returns it to "process_urls_from_file". Then the function calls"scrape_article" which uses BeautifulSoup to find the article text through the URL's HTML and stores it into a new variable then returns it. Finally, "process_urls_from_file" calls "save_article" which simply creates a folder for the article files if ther isn't already one and then stores a new file per URL into that folder with all of the article content on it.  

To use this program, simply have a file with the URLS, in which you would like to be scrapped, named "page.txt". Each URL must be in its own line. Also, make sure you are using python 3.XX and have both BeautifulSoup and Requirements libraries downloaded. To open the environment, type this command into your terminal, conda env create -f requirements.yaml. You need to make sure that all your files are in the same pathway. If the URLs are from a different site than CNN Sports minor changes my have to be made to line 22 depending on where the text is stored in the URL's HTML. To see the URL's HTML on Mac OS use command+option+i. Once all that is done, simply run the program and watch as your scrapped URL files are created. 